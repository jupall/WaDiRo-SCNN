# WaDiRo-SCNN
A Python implementation of the Wasserstein Distributionally Robust Shallow Convex Neural Networks from the work of Julien Pallage and Antoine Lesage-Landry.

üöß **Page in construction** üöß
---
---
## Introduction:
This work generalizes prior work done by [1,2] under the lens of Wasserstein Distributionally Robust Optimization. We train our model with an exact reformulation of the order-1 Wasserstein DRO problem [3,4,5].

Why is it interesting? :
- Non-linear scalable predictor;
- Low-stochasticity training compared to standard Shallow Neural Networks training programs.
- Provable out-of-sample performance: perfect for critical applications, e.g., energy, finance, healthcare;
- Easily solvable with open-source solvers, e.g., Clarabel [6];
- Conservative training procedure which generalizes standard regularization;
- Possibility to enforce hard physical constraints in training (physics-constrained NN).

## How it is made:
We use `cvxpy` to solve the models and convert the neural networks into `PyTorch`'s framework. 

In addition to the WaDiRo-SCNN, we also implement a WaDiRo linear regression and our own version of the SCNN.

## How to use it:

### Download:
```python
pip install wadiroscnn
```

### Create and train a model:

Choose between:
1. `wadiro_scnn()`
2. `wadiro_linreg()`
3. `scnn()`

```python
solver_name = "CLARABEL" # default option
verbose = True 

radius = 0.01 # the Wasserstein ball's radius
bias = True # construct the model with bias weights
wasserstein = "l2" # choose the norm used in the definition of the Wasserstein distance

model = wadiro_scnn()
model.train(X_train=data.X_train, Y_train=data.Y_train, radius = radius, bias = bias, max_neurons=max_neurons, verbose=verbose, solver=solver_name, wasserstein=wasserstein)
```


### Convert into a PyTorch neural network and predict:

```python

model_torch = model.get_torch_model(verbose = False)

output = model_torch(X_test)

```

## Cite our work and read our paper:

COMING SOON

## References:

[1] M. Pilanci and T. Ergen, ‚ÄúNeural networks are convex regularizers: Exact polynomial-time convex optimization formulations for two-layer networks,‚Äù in International Conference on Machine Learning, PMLR, 2020, pp. 7695‚Äì7705.

[2] A. Mishkin, A. Sahiner, and M. Pilanci, ‚ÄúFast Convex Optimization for Two-Layer ReLU Networks: Equivalent Model Classes and Cone Decompositions,‚Äù in International Conference on Machine Learning, 2022.

[3] P. Mohajerin Esfahani and D. Kuhn, ‚ÄúData-driven distributionally robust optimization using the Wasserstein metric: Performance guarantees and tractable reformulations,‚Äù Mathematical Programming, vol. 171, no. 1, pp. 115‚Äì166, 2018.

[4] D. Kuhn, P. M. Esfahani, V. A. Nguyen, and S. Shafieezadeh-Abadeh, ‚ÄúWasserstein distributionally robust optimization: Theory and applications in machine learning,‚Äù in Operations research & management science in the age of analytics, Informs, 2019, pp. 130‚Äì166.

[5] R. Chen and I. Ch. Paschalidis, ‚ÄúDistributionally Robust Learning,‚Äù Foundations and Trends¬Æ in Optimization, vol. 4, no. 1-2, pp. 1‚Äì243, 2020s

[6] P. J. Goulart and Y. Chen, ‚ÄúClarabel: An interior-point solver for conic programs with quadratic objectives,‚Äù Department of Engineering Science, University of Oxford, 2024.

## Acknowledgements:

This work was possible thanks to:

1. the incredible python package pyscnn : https://github.com/pilancilab/scnn
2. the open-source convex solver Clarabel : https://github.com/oxfordcontrol/Clarabel.jl
3. the library benchmark functions : https://gitlab.com/luca.baronti/python_benchmark_functions/-/tree/master?ref_type=heads
4. CVXPY : https://www.cvxpy.org/version/1.1/index.html


