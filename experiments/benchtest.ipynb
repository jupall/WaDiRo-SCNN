{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/wadiroscnn')\n",
    "import hyperopt as hp\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import models as hm\n",
    "import benchmark_functions as bf\n",
    "import benchmarking_functions as bench\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK, Trials\n",
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "import cvxpy as cp\n",
    "import mlflow.pytorch\n",
    "from mlflow.models.signature import infer_signature, set_signature\n",
    "import hyperopt\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run this from terminal :\n",
    "\n",
    "mlflow server --host 127.0.0.1 --port 8081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8081\")\n",
    "# Create a new MLflow Experiment\n",
    "experiment = mlflow.set_experiment(\"name_experiment\")\n",
    "\n",
    "# Get Experiment Details\n",
    "print(\"Experiment_id: {}\".format(experiment.experiment_id))\n",
    "print(\"Artifact Location: {}\".format(experiment.artifact_location))\n",
    "print(\"Name: {}\".format(experiment.name))\n",
    "print(\"Tags: {}\".format(experiment.tags))\n",
    "print(\"Lifecycle_stage: {}\".format(experiment.lifecycle_stage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "search_space_DR_SCNN = {'radius': hp.loguniform('radius',-8, 1),\n",
    "                'max_neurons': hp.quniform('max_neurons', 10, 300, 1),\n",
    "                'wasserstein': hp.choice('wasserstein', [\"l1\", \"l2\"]),\n",
    "                }\n",
    "\n",
    "search_space_SCNN = {'lamb_reg': hp.loguniform('lamb_reg',-8, 1),\n",
    "                'max_neurons': hp.quniform('max_neurons', 10, 300, 1),\n",
    "                'regularizer': hp.choice('regularizer', [\"LASSO\", \"RIDGE\"]),\n",
    "                }\n",
    "\n",
    "search_space_SCNN_no_reg = {#'lamb_reg': hp.loguniform('lamb_reg',-8, 1),\n",
    "                'max_neurons': hp.quniform('max_neurons', 10, 300, 1),\n",
    "                }\n",
    "\n",
    "\n",
    "search_space_DR_linreg = {'radius': hp.loguniform('radius',-8, 1),\n",
    "                          'wasserstein': hp.choice('wasserstein', [\"l1\", \"l2\"]),\n",
    "                }\n",
    "\n",
    "search_space_linreg = {'lamb_reg': hp.loguniform('lamb_reg',-8, 1),\n",
    "                'regularizer': hp.choice('regularizer', [\"LASSO\",\"RIDGE\"]),\n",
    "                }\n",
    "\n",
    "search_space_FNN =  {'batch_size' : hp.quniform('batch_size',2, 100, 1),\n",
    "                'learning_rate': hp.loguniform('learning_rate',-8, 1),\n",
    "                'n_hidden' : hp.quniform('n_hidden',10, 300, 1),\n",
    "                'n_epochs': hp.quniform( 'n_epochs',1, 1000, 1),\n",
    "                'dropout_p': hp.uniform('dropout_p', 0.01, 0.4)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "solver_name = \"CLARABEL\"\n",
    "max_evals = 400\n",
    "max_evals_FNN = int(max_evals/4)\n",
    "verbose = False\n",
    "N = 2000\n",
    "train_size = 0.75\n",
    "test_size = 0.2\n",
    "n_corrupted_points = int(N*0.8*0.1) #int(0.6*N)\n",
    "n_dim = 4\n",
    "\n",
    "FUNCTIONS_LIST = [ \n",
    "\t[bf.Ackley, n_dim, \"Ackley\"],\t\t\t\n",
    "\t[bf.Keane,n_dim, \"Keane\"],\n",
    "\t[bf.PichenyGoldsteinAndPrice,2, \"PichenyGoldsteinAndPrice\"],\n",
    "\t[bf.McCormick, 2, \"McCormick\"], # 2 dims\n",
    "\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for benchmark_func in FUNCTIONS_LIST:\n",
    "    data = bench.dataset_wrapper(benchmark_func=benchmark_func[0], n_dim=benchmark_func[1], rng =rng, N = N, train_size=train_size, test_size=test_size)\n",
    "    func_name = benchmark_func[2]\n",
    "    data.generate_data_wasserstein_corrupt_both(corrupt_data_points= n_corrupted_points, min_distance=0.05, n_projections=100, k_multiplier=1.5, L=2, seed=42)\n",
    "\n",
    "\n",
    "    print(f\"SCNN_no_reg on {func_name}: \\n\")\n",
    "    fmin_SCNN_no_reg = partial(bench.objective_SCNN_no_reg, data = data, criterion = criterion, solver_name = solver_name, experiment=experiment, n_corrupted_points = n_corrupted_points, func_name=func_name, verbose=verbose)\n",
    "    argmin_SCNN_no_reg = fmin(fn=fmin_SCNN_no_reg,\n",
    "              space=search_space_SCNN_no_reg,\n",
    "              algo= hyperopt.anneal.suggest, #hyperopt.tpe.suggest, # try anneal.suggest\n",
    "              max_evals=int(max_evals)) #trials=spark_trials)\n",
    "    \n",
    "    print(f\"WaDiRO SCNN on {func_name}: \\n\")\n",
    "    fmin_DR_SCNN = partial(bench.objective_DR_SCNN, data = data, criterion = criterion, solver_name = solver_name, experiment=experiment, n_corrupted_points=n_corrupted_points, func_name=func_name, verbose=verbose)\n",
    "    argmin_DR_SCNN = fmin(fn=fmin_DR_SCNN,\n",
    "              space=search_space_DR_SCNN,\n",
    "              algo= hyperopt.anneal.suggest, #hyperopt.tpe.suggest, # try anneal.suggest\n",
    "              max_evals=max_evals) #trials=spark_trials)\n",
    "    \n",
    "    print(f\"SCNN on {func_name}: \\n\")\n",
    "    fmin_SCNN = partial(bench.objective_SCNN, data = data, criterion = criterion, solver_name = solver_name, experiment=experiment, n_corrupted_points = n_corrupted_points, func_name=func_name, verbose=verbose)\n",
    "    argmin_SCNN = fmin(fn=fmin_SCNN,\n",
    "              space=search_space_SCNN,\n",
    "              algo= hyperopt.anneal.suggest, #hyperopt.tpe.suggest, # try anneal.suggest\n",
    "              max_evals=max_evals) #trials=spark_trials)\n",
    "    \n",
    "    print(f\"linreg on {func_name}: \\n\")\n",
    "    fmin_linreg = partial(bench.objective_linreg, data = data, criterion = criterion, solver_name = solver_name, experiment=experiment, n_corrupted_points=n_corrupted_points,  func_name=func_name, verbose=verbose)\n",
    "    argmin_linreg = fmin(fn=fmin_linreg,\n",
    "              space=search_space_linreg,\n",
    "              algo= hyperopt.anneal.suggest,#hyperopt.tpe.suggest, # try anneal.suggest\n",
    "              max_evals=max_evals) #trials=spark_trials)\n",
    "\n",
    "    print(f\"WaDiRo linreg on {func_name}: \\n\")\n",
    "    fmin_DR_linreg = partial(bench.objective_DR_linreg, data = data, criterion = criterion, solver_name = solver_name, experiment=experiment, n_corrupted_points=n_corrupted_points,  func_name=func_name, verbose=verbose)\n",
    "    argmin_DR_linreg = fmin(fn=fmin_DR_linreg,\n",
    "              space=search_space_DR_linreg,\n",
    "              algo= hyperopt.anneal.suggest,#hyperopt.tpe.suggest, # try anneal.suggest\n",
    "              max_evals=max_evals) #trials=spark_trials)\n",
    "              \n",
    "    \n",
    "    print(f\"FNN on {func_name}: \\n\")\n",
    "    fmin_FNN = partial(bench.objective_FNN, data = data, criterion = criterion, experiment=experiment, n_corrupted_points=n_corrupted_points,  func_name=func_name, verbose=verbose)\n",
    "    argmin_FNN = fmin(fn=fmin_FNN,\n",
    "              space=search_space_FNN,\n",
    "              algo= hyperopt.anneal.suggest, # try anneal.suggest\n",
    "              max_evals=max_evals_FNN) #trials=spark_trials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mlflow.search_runs([experiment.experiment_id])\n",
    "#building_ids = df['params.building_id']\n",
    "display(df)\n",
    "functions = df['params.benchmark_functions'].unique()\n",
    "print(functions)\n",
    "models = ['DR_SCNN',  'SCNN', 'SCNN_no_reg', 'FNN',  'DR_linreg', 'linreg' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result_dict = {}\n",
    "\n",
    "for function in functions:\n",
    "    if function != 'DeJong5' and function != 'Schwefel' and function != None:\n",
    "        for model in models:\n",
    "            best_mae = df.loc[(df['params.benchmark_functions'] == function) & (df['tags.model_name'] == model)].sort_values(by='metrics.MAE_val', ascending=True).head(1)\n",
    "            best_mse = df.loc[(df['params.benchmark_functions'] == function) & (df['tags.model_name'] == model)].sort_values(by='metrics.MSE_val', ascending=True).head(1)\n",
    "            #best_build = df.loc[ (df['tags.model_name'] == model)].sort_values(by='metrics.MAE_VAL_all_buildings_scaled', ascending=True).head(1)\n",
    "\n",
    "            if f'{function}' not in result_dict.keys():\n",
    "                result_dict[f'{function}'] = {}\n",
    "        \n",
    "            #result_dict[f'{function}'][f'{model}'] = {'best_mae': best_mae['metrics.MAE_test'].values[0], 'best_mse': best_mse['metrics.MSE_test'].values[0]}\n",
    "            result_dict[f'{function}'][f'{model}'] = {'best_mae': best_mae['metrics.MAE_test'].values[0], 'best_mse': best_mae['metrics.MSE_test'].values[0], 'run_id': best_mae['run_id'].values[0]}\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.concat({\n",
    "        k: pd.DataFrame.from_dict(v, 'index') for k, v in result_dict.items()\n",
    "    }, \n",
    "    axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['best_mae']['Ackley'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'Model': ['WaDiRo SCNN', 'SCNN with regularization',  'SCNN no regularization', 'Deep FNN', 'WaDiRo linear reg.', 'Linear reg. with regularization',  ],\n",
    "    'McCormick MAE': df_2['best_mae']['McCormick'].values,\n",
    "    'McCormick MSE': df_2['best_mse']['McCormick'].values,\n",
    "    'PGandP MAE': df_2['best_mae']['PichenyGoldsteinAndPrice'].values,\n",
    "    'PGandP MSE': df_2['best_mse']['PichenyGoldsteinAndPrice'].values,\n",
    "    'Keane MAE': df_2['best_mae']['Keane'].values,\n",
    "    'Keane MSE': df_2['best_mse']['Keane'].values,\n",
    "    'Ackley MAE': df_2['best_mae']['Ackley'].values,\n",
    "    'Ackley MSE':  df_2['best_mse']['Ackley'].values\n",
    "}).set_index('Model')\n",
    "\n",
    "# Normalize each column by its range\n",
    "data_norm = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "plt.figure(figsize=(10, 6), dpi = 400)\n",
    "#plt.tight_layout()\n",
    "cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "heatmap = sns.heatmap(data_norm, cmap=cmap, annot=True, fmt=\".2f\",cbar_kws={'label': 'Normalized error'})\n",
    "\n",
    "# Set the function names as the y-axis labels\n",
    "heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0)\n",
    "\n",
    "# Set the function names as the x-axis labels and place them on top\n",
    "heatmap.set_xticklabels(data.columns, rotation=45)\n",
    "heatmap.xaxis.tick_top()\n",
    "\n",
    "#plt.title('Your caption here', y=-0.1)\n",
    "#plt.title('Normalized error metrics for different models and benchmark functions')\n",
    "plt.savefig('heatmap_JOPT.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6), dpi = 400)\n",
    "#plt.tight_layout()\n",
    "cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "heatmap = sns.heatmap(data, cmap=cmap, annot=True, fmt=\".2f\",cbar_kws={'label': 'Error'})\n",
    "\n",
    "# Set the function names as the y-axis labels\n",
    "heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0)\n",
    "\n",
    "# Set the function names as the x-axis labels and place them on top\n",
    "heatmap.set_xticklabels(data.columns, rotation=45)\n",
    "heatmap.xaxis.tick_top()\n",
    "\n",
    "#plt.title('Your caption here', y=-0.1)\n",
    "#plt.title('Normalized error metrics for different models and benchmark functions')\n",
    "plt.savefig('heatmap_JOPT_corruptBoth_bit_less_MAE.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
